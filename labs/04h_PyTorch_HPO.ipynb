{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d430fffc-5336-4fe3-b498-53e5e60218c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Torch implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6539550e-503d-4c2a-aa27-6df14b5633f1",
   "metadata": {},
   "source": [
    "The `PytorchClassifier` is a custom class that is part of the sklearn-wrap library, which is a library that allows you to use PyTorch models in the scikit-learn framework. Here is an example of how the PyTorchClassifier class can be defined:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a0cf5a5-0d68-4c9a-88c4-1789a071858a",
   "metadata": {},
   "source": [
    "- The `PyTorchClassifier` class is a simple implementation of a classifier that uses a PyTorch model. It has three main methods: `fit`, which trains the model on the given data, `predict`, which makes predictions on new data, and `score`, which computes the accuracy of the model on the given data. \n",
    "- The `fit` method takes the data as input and trains the model using the `create_model` function, the `batch_size`, and the `learning_rate` hyperparameters. \n",
    "- The `predict` method takes the data as input and makes predictions using the trained model. \n",
    "- The `score` method takes the data and the ground truth labels as input, and computes the accuracy of the model on the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8ec47a0b-d6d4-4db1-a7a1-688a6b29e00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PyTorchClassifier:\n",
    "  def __init__(self, create_model, batch_size=10, learning_rate=0.001, num_epochs=100):\n",
    "    self.create_model = create_model\n",
    "    self.batch_size = batch_size\n",
    "    self.learning_rate = learning_rate\n",
    "    self.num_epochs = num_epochs\n",
    "    self.model = None\n",
    "    self.criterion = None\n",
    "    self.optimizer = None\n",
    "\n",
    "  def fit(self, X, y):\n",
    "    # Create the model, loss function, and optimizer\n",
    "    self.model, self.criterion, self.optimizer = self.create_model(self.batch_size, self.learning_rate)\n",
    "\n",
    "    # Convert the data to PyTorch tensors\n",
    "    X = torch.Tensor(X)\n",
    "    y = torch.Tensor(y).long()\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(self.num_epochs):\n",
    "      # Forward pass\n",
    "      y_pred = self.model(X)\n",
    "\n",
    "      # Compute the loss\n",
    "      loss = self.criterion(y_pred, y)\n",
    "\n",
    "      # Zero the gradients\n",
    "      self.optimizer.zero_grad()\n",
    "\n",
    "      # Backward pass\n",
    "      loss.backward()\n",
    "\n",
    "      # Update the weights\n",
    "      self.optimizer.step()\n",
    "\n",
    "  def predict(self, X):\n",
    "    # Convert the data to PyTorch tensors\n",
    "    X = torch.Tensor(X)\n",
    "\n",
    "    # Forward pass\n",
    "    y_pred = self.model(X)\n",
    "\n",
    "    # Convert the predictions to numpy arrays\n",
    "    return y_pred.detach().numpy()\n",
    "\n",
    "  def score(self, X, y):\n",
    "    # Make predictions\n",
    "    y_pred = self.predict(X)\n",
    "\n",
    "    # Compute the accuracy\n",
    "    accuracy = (y_pred.argmax(axis=1) == y).mean()\n",
    "    return accuracy\n",
    "\n",
    "  def get_params(self, deep=True):\n",
    "    return {\n",
    "          'create_model': self.create_model,\n",
    "          'batch_size': self.batch_size,\n",
    "          'learning_rate': self.learning_rate,\n",
    "          'num_epochs': self.num_epochs\n",
    "   }\n",
    "\n",
    "  def set_params(self, **params):\n",
    "    self.create_model = params['create_model']\n",
    "    self.batch_size = params['batch_size']\n",
    "    self.learning_rate = params['learning_rate']\n",
    "    self.num_epochs = params['num_epochs']\n",
    "    return self\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fef2064",
   "metadata": {},
   "source": [
    "## `sklearn` interface for `GridSearchCV`\n",
    "- In this code, we first use the `load_iris` function from the `sklearn` library to load the iris dataset into memory. We then standardize the features using the `StandardScaler` class from the `sklearn` library. \n",
    "- Next, we define the hyperparameters to be optimized, which are `batch_size` and `learning_rate`, as in the previous example. We then define the `create_model` function to create a simple model with two linear layers and one output layer. \n",
    "- The output layer uses the `Softmax` activation function since this is a multi-class classification problem. \n",
    "- We then create a `PyTorchClassifier` object and use the `GridSearchCV` function to find the optimal set of hyperparameters. Finally, we print the optimal hyperparameters using the `best_params_` attribute of the `GridSearchCV` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6b11e6d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "PyTorchClassifier.__init__() missing 1 required positional argument: 'create_model'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 56\u001b[0m\n\u001b[1;32m     54\u001b[0m grid_search \u001b[39m=\u001b[39m GridSearchCV(model, param_grid\u001b[39m=\u001b[39mparam_grid, cv\u001b[39m=\u001b[39mStratifiedKFold(n_splits\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m))\n\u001b[1;32m     55\u001b[0m \u001b[39m# param_grid=dict(batch_size=batch_size, epochs=epochs, learning_rate=learning_rate)\u001b[39;00m\n\u001b[0;32m---> 56\u001b[0m grid_search\u001b[39m.\u001b[39;49mfit(X, y)\n\u001b[1;32m     58\u001b[0m \u001b[39m# Print the optimal set of hyperparameters\u001b[39;00m\n\u001b[1;32m     59\u001b[0m \u001b[39mprint\u001b[39m(grid_search\u001b[39m.\u001b[39mbest_params_)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/model_selection/_search.py:789\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    786\u001b[0m cv_orig \u001b[39m=\u001b[39m check_cv(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcv, y, classifier\u001b[39m=\u001b[39mis_classifier(estimator))\n\u001b[1;32m    787\u001b[0m n_splits \u001b[39m=\u001b[39m cv_orig\u001b[39m.\u001b[39mget_n_splits(X, y, groups)\n\u001b[0;32m--> 789\u001b[0m base_estimator \u001b[39m=\u001b[39m clone(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mestimator)\n\u001b[1;32m    791\u001b[0m parallel \u001b[39m=\u001b[39m Parallel(n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs, pre_dispatch\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpre_dispatch)\n\u001b[1;32m    793\u001b[0m fit_and_score_kwargs \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(\n\u001b[1;32m    794\u001b[0m     scorer\u001b[39m=\u001b[39mscorers,\n\u001b[1;32m    795\u001b[0m     fit_params\u001b[39m=\u001b[39mfit_params,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    801\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[1;32m    802\u001b[0m )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/sklearn/base.py:88\u001b[0m, in \u001b[0;36mclone\u001b[0;34m(estimator, safe)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[39mfor\u001b[39;00m name, param \u001b[39min\u001b[39;00m new_object_params\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     87\u001b[0m     new_object_params[name] \u001b[39m=\u001b[39m clone(param, safe\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 88\u001b[0m new_object \u001b[39m=\u001b[39m klass(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mnew_object_params)\n\u001b[1;32m     89\u001b[0m params_set \u001b[39m=\u001b[39m new_object\u001b[39m.\u001b[39mget_params(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m     91\u001b[0m \u001b[39m# quick sanity check of the parameters of the clone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: PyTorchClassifier.__init__() missing 1 required positional argument: 'create_model'"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from torch.nn import Linear\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load the iris dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Define the hyperparameters to be optimized\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100] # Added based on KerasClassifier\n",
    "learning_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "\n",
    "# Define a function to create the model\n",
    "def create_model(batch_size, learning_rate):\n",
    "  model = nn.Sequential(\n",
    "      Linear(4, 8),\n",
    "      nn.ReLU(),\n",
    "      Linear(8, 3),\n",
    "      nn.Softmax()\n",
    "  )\n",
    "  criterion = nn.CrossEntropyLoss()\n",
    "  optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "  return model, criterion, optimizer\n",
    "\n",
    "# Create a PyTorch classifier\n",
    "model = PyTorchClassifier(create_model, batch_size=batch_size, learning_rate=learning_rate)\n",
    "\n",
    "# Create a Keras classifier\n",
    "# ALT\n",
    "# model = KerasClassifier(build_fn=create_model, verbose=0)\n",
    "# Modify create_model with:\n",
    "# def create_model():\n",
    "  #model = Sequential()\n",
    "  #model.add(Dense(12, input_dim=8, activation='relu'))\n",
    "  #model.add(Dense(1, activation='sigmoid'))\n",
    "  #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  #return model\n",
    "\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {'batch_size': batch_size, 'epochs': epochs, 'learning_rate': learning_rate}\n",
    "\n",
    "# Use grid search to find the optimal set of hyperparameters\n",
    "grid_search = GridSearchCV(model, param_grid=param_grid, cv=StratifiedKFold(n_splits=3))\n",
    "# param_grid=dict(batch_size=batch_size, epochs=epochs, learning_rate=learning_rate)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the optimal set of hyperparameters\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36e9c112",
   "metadata": {},
   "source": [
    "\n",
    "# Keras implementation:\n",
    "\n",
    "In this code, we are using the `KerasClassifier` class from the Keras library to create a deep learning model. The `KerasClassifier` class takes a function that defines the architecture of the model as an argument, and we use the create_model function to create a simple model with two dense layers and one output layer. \n",
    "We are optimizing the hyperparameters `batch_size` and `epochs`, which control the size of the mini-batches used for training and the number of training iterations, respectively. The `GridSearchCV` function trains the model using different combinations of these hyperparameters, and then finds the combination that gives the best performance on the training data. The optimal hyperparameters are then printed using the `best_params_` attribute of the `GridSearchCV` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "120449b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'batch_size': 10, 'epochs': 10}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.datasets import load_iris\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "# TF Keras wrappers is deprecated - use above\n",
    "# from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the iris dataset\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Define the hyperparameters to be optimized\n",
    "batch_size = [10, 20, 40, 60, 80, 100]\n",
    "epochs = [10, 50, 100] \n",
    "learning_rate = [0.001, 0.01, 0.1, 0.2, 0.3]\n",
    "\n",
    "# Define a function to create the model\n",
    "def create_model():\n",
    "  model = Sequential()\n",
    "  model.add(Dense(12, input_dim=4, activation='relu'))\n",
    "  model.add(Dense(1, activation='sigmoid'))\n",
    "  model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "  return model\n",
    "\n",
    "# Create a PyTorch classifier\n",
    "#model = KerasClassifier(build_fn=create_model, verbose=0) # build_fn is deprecated. Use `model`\n",
    "model = KerasClassifier(model=create_model, verbose=0)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {'batch_size': batch_size, 'epochs': epochs}\n",
    "\n",
    "# Use grid search to find the optimal set of hyperparameters\n",
    "grid_search = GridSearchCV(model, param_grid=param_grid, cv=StratifiedKFold(n_splits=3))\n",
    "# param_grid=dict(batch_size=batch_size, epochs=epochs, learning_rate=learning_rate)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the optimal set of hyperparameters\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b0ef88",
   "metadata": {},
   "source": [
    "## Using RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "146912c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 5, 'n_estimators': 100}\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define the hyperparameters to be optimized\n",
    "hyperparameters = {'n_estimators': [10, 100, 500, 1000],\n",
    "                    'max_depth': [5, 10, 15, 20]}\n",
    "\n",
    "# Create a random forest classifier with default parameters\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# Use grid search to find the optimal set of hyperparameters\n",
    "grid_search = GridSearchCV(clf, hyperparameters, cv=3)\n",
    "grid_search.fit(X, y)\n",
    "\n",
    "# Print the optimal set of hyperparameters\n",
    "print(grid_search.best_params_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad43f531",
   "metadata": {},
   "source": [
    "# ResNet50 implementatation in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc523691",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8e35007",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(ResNet50, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = self._make_layer(64, 64, 3)\n",
    "        self.layer2 = self._make_layer(64, 128, 4, stride=2)\n",
    "        self.layer3 = self._make_layer(128, 256, 6, stride=2)\n",
    "        self.layer4 = self._make_layer(256, 512, 3, stride=2)\n",
    "\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(512 * 4, num_classes)\n",
    "\n",
    "    def _make_layer(self, in_channels, out_channels, num_blocks, stride=1):\n",
    "        layers = []\n",
    "        for i in range(num_blocks):\n",
    "            if i == 0:\n",
    "                layers.append(ResidualBlock(in_channels, out_channels, stride=stride))\n",
    "            else:\n",
    "                layers.append(ResidualBlock(out_channels, out_channels))\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ad63f449",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        # Define the first convolutional layer, which has a kernel size of 3, stride of `stride`, and padding of 1\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        # Define the batch normalization layer for the first convolutional layer\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        # Define the ReLU activation function\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        # Define the second convolutional layer, which has the same configuration as the first layer\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        # Define the batch normalization layer for the second convolutional layer\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # If the stride is not 1 or the number of input channels is not equal to the number of output channels,\n",
    "        # a downsampling operation is performed to match the dimensions of the input and output tensors\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels),\n",
    "            )\n",
    "        else:\n",
    "            self.downsample = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # If a downsampling operation is performed, apply it to the input tensor\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        else:\n",
    "            residual = x\n",
    "\n",
    "        # Apply the first convolutional layer, batch normalization, and ReLU activation\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        # Apply the second convolutional layer and batch normalization\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        # Add the residual tensor to the output of the second convolutional layer and apply the ReLU activation\n",
    "        x += residual\n",
    "        x = self.relu(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d424e7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
