{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "yellow-interface",
   "metadata": {},
   "source": [
    "# Character RNN\n",
    "\n",
    "We use an updated version of Andrej Karpathy's seminal [blog](http://karpathy.github.io/2015/05/21/rnn-effectiveness) - The unusual effectiveness of RNN ([gist](https://gist.github.com/karpathy/d4dee566867f8291f086) | [code](https://github.com/karpathy/char-rnn)) and an updated code [here](https://render.githubusercontent.com/view/sessions/RNN.ipynb). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "exposed-grammar",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "from random import uniform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2205c37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘dataset’: File exists\r\n"
     ]
    }
   ],
   "source": [
    "!mkdir dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "military-minimum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-09 14:57:15--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 1115394 (1.1M) [text/plain]\n",
      "Saving to: 'dataset/input.txt'\n",
      "\n",
      "dataset/input.txt   100%[===================>]   1.06M  3.92MB/s    in 0.3s    \n",
      "\n",
      "2021-03-09 14:57:16 (3.92 MB/s) - 'dataset/input.txt' saved [1115394/1115394]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -O dataset/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "premium-digit",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of characters: 1115394\n",
      "Number of unique characters: 65\n",
      "Number of lines: 40000\n",
      "Number of words: 169892\n",
      "\n",
      "Excerpt:\n",
      "********\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor\n"
     ]
    }
   ],
   "source": [
    "filename = 'dataset/input.txt'\n",
    "text = open(filename, 'rt').read()\n",
    "print(\"Number of characters: {}\".format(len(text)))\n",
    "print(\"Number of unique characters: {}\".format(len(set(text))))\n",
    "print(\"Number of lines: {}\".format(text.count('\\n')))\n",
    "print(\"Number of words: {}\".format(text.count(' ')))\n",
    "print()\n",
    "print(\"Excerpt:\")\n",
    "print(\"*\" * len(\"Excerpt:\"))\n",
    "print(text[:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "lesbian-bridges",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the input text to characters\n",
    "chars = list(set(text))\n",
    "\n",
    "# total characters and unique character size\n",
    "data_size, vocab_size = len(text), len(chars) \n",
    "\n",
    "# dict from index to char and vice-versa - converted to numpy\n",
    "idx_to_char = dict(enumerate(chars)) # { i: ch for i,ch in enumerate(chars) }\n",
    "char_to_idx = dict(zip(idx_to_char.values(), idx_to_char.keys())) # { ch: i for i,ch in enumerate(chars) }\n",
    "data = np.array([char_to_idx[c] for c in text], dtype=int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-purple",
   "metadata": {},
   "source": [
    "Defining the softmax and loss functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "phantom-polymer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x = x.squeeze()\n",
    "    expx = np.exp(x - x.sum())\n",
    "    return expx / expx.sum()\n",
    "\n",
    "def cross_entropy(predictions, targets):\n",
    "    return sum([-np.log(p[t]) for p, t in zip(predictions, targets)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "obvious-mouth",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimization code \n",
    "def average(prev, curr, β):\n",
    "    return [\n",
    "        β * p + (1 - β) * c\n",
    "        for p, c\n",
    "        in zip(prev, curr)\n",
    "    ]\n",
    "    \n",
    "class AdamOptimizer:\n",
    "    def __init__(self, α=0.001, β1=0.9, β2=0.999, ϵ=1e-8):\n",
    "        self.α = α\n",
    "        self.β1 = β1\n",
    "        self.β2 = β2\n",
    "        self.ϵ = ϵ\n",
    "        self.m = None\n",
    "        self.v = None\n",
    "        self.t = 0\n",
    "\n",
    "    def send(self, gradients):\n",
    "        if self.m is None:\n",
    "            self.m = [0] * len(gradients)\n",
    "        if self.v is None:\n",
    "            self.v = [0] * len(gradients)\n",
    "\n",
    "        self.t += 1\n",
    "        αt = self.α * np.sqrt(1 - self.β2**self.t) / (1 - self.β1**self.t)\n",
    "        self.m = average(self.m, gradients, self.β1)        \n",
    "        self.v = average(self.v, (g*g for g in gradients), self.β2)\n",
    "\n",
    "        updates = [-αt * mi / (np.sqrt(vi) + self.ϵ) for mi, vi in zip(self.m, self.v)]\n",
    "        for upd in updates:\n",
    "            assert np.isfinite(upd).all()\n",
    "        return updates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "recent-header",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step(params, x_t, h_t_1=None):\n",
    "    Wxh, Whh, Why, bh, by = params\n",
    "    if h_t_1 is None:\n",
    "        h_t_1 = np.zeros(h_size)    \n",
    "    if h_t_1.ndim == 1:\n",
    "        h_t_1 = h_t_1.reshape(-1, 1)\n",
    "    if x_t.ndim == 1:\n",
    "        x_t = x_t.reshape(-1, 1)\n",
    "\n",
    "    # update hidden layer\n",
    "    h_t = np.tanh(Wxh @ x_t + Whh @ h_t_1 + bh)\n",
    "    # fully connected layer\n",
    "    z_t = Why @ h_t + by\n",
    "    z_t = z_t.squeeze()\n",
    "    h_t = h_t.squeeze()\n",
    "    # softmax readout layer\n",
    "    yhat_t = softmax(z_t)\n",
    "    return h_t, z_t, yhat_t\n",
    "\n",
    "def feed_forward(params, x, h0=None):\n",
    "    if h0 is None:\n",
    "        h0 = np.zeros(h_size)\n",
    "    h = {-1: h0}\n",
    "    \n",
    "    shape = (len(x), vocab_size)\n",
    "    x_original = x.copy()\n",
    "    x, z, yhat = np.zeros(shape), np.empty(shape), np.empty(shape)\n",
    "    \n",
    "    for t, char_idx in enumerate(x_original):\n",
    "        x[t, char_idx] = 1.0 # one-hot encoding input into xs  \n",
    "        h[t], z[t, :], yhat[t, :] = step(params, x[t, :], h[t-1])\n",
    "\n",
    "    return x, h, z, yhat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "viral-geneva",
   "metadata": {},
   "outputs": [],
   "source": [
    "def back_propagation(params, x, y, h0=None):\n",
    "    \"\"\"Calculates loss and gradiens of loss wrt paramters\n",
    "    \n",
    "    See http://cs231n.github.io/neural-networks-case-study/#grad\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    params : list of arrays\n",
    "        model parameters\n",
    "    x, y : list of integers\n",
    "        indices of characters for the input and target of the network\n",
    "    h0 : np.ndarray\n",
    "        initial hidden state of shape Hx1\n",
    "    Returns\n",
    "    -------\n",
    "    loss : float\n",
    "        value of loss function\n",
    "    dWxh, dWhh, dWhy, dbh, dby \n",
    "        gradients of the loss function wrt to model parameters\n",
    "    h0 : np.ndarray\n",
    "        initial hidden state\n",
    "    \"\"\"\n",
    "    n_inputs = len(x)\n",
    "    # forward pass: compute predictions and loss going forwards\n",
    "    x, h, z, yhat = feed_forward(params, x, h0=h0)\n",
    "    loss = cross_entropy(yhat, y)\n",
    "    \n",
    "    # backward pass: compute gradients going backwards\n",
    "    Wxh, Whh, Why, bh, by = params\n",
    "    dWxh, dWhh, dWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "    dbh, dby = np.zeros_like(bh), np.zeros_like(by)\n",
    "    dh_next = np.zeros_like(h[0])\n",
    "    \n",
    "    # back propagate through the unrolled network\n",
    "    for t in reversed(range(len(y))):\n",
    "        # backprop into y\n",
    "        x_t, h_t, yhat_t, y_t = x[t], h[t], yhat[t], y[t] # can't zip because hs is not ordered\n",
    "        dyhat = yhat_t.copy()\n",
    "        dyhat[y_t] -= 1 # Yhat - Y\n",
    "        dWhy += np.outer(dyhat, h_t)  # outer product, same as dy.reshape(-1, 1) @ h.reshape(1, -1)\n",
    "        dby += dyhat.reshape(-1, 1) # dby is a column vector\n",
    "        # backprop into h_t\n",
    "        dh = Why.T @ dyhat + dh_next\n",
    "        # backprop through tanh\n",
    "        dh = (1 - h_t * h_t) * dh # tanh'(x) = 1-x^2\n",
    "        dbh += dh.reshape(-1, 1) # dbh is a column vector\n",
    "        dWxh += np.outer(dh, x_t)\n",
    "        dWhh += np.outer(dh, h[t-1]) # try to use h[t] instead of h[t-1] and see effect in grad_check\n",
    "        dh_next = Whh.T @ dh\n",
    "\n",
    "    gradients = dWxh, dWhh, dWhy, dbh, dby\n",
    "    for grad in gradients:\n",
    "        # clip to mitigate exploding gradients\n",
    "        np.clip(grad, -5, 5, out=grad) # out=grad makes this run in-place\n",
    "    return loss, gradients, h[n_inputs-1]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-treaty",
   "metadata": {},
   "source": [
    "## Trainer\n",
    "\n",
    "Data in batches of `seq_length` and calculation of gradients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "superior-typing",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "    def __init__(self, data, seq_length):\n",
    "        self.optimizer = AdamOptimizer()\n",
    "        self.step, self.pos, self.h = 0, 0, None\n",
    "        self.seq_length = seq_length\n",
    "        self.data = data\n",
    "\n",
    "    def train(self, params):\n",
    "        self.step += 1\n",
    "        if self.pos + self.seq_length + 1 >= len(self.data):\n",
    "            # reset data position and hidden state\n",
    "            self.pos, self.h = 0, None\n",
    "        x = self.data[self.pos : self.pos + self.seq_length]\n",
    "        y = self.data[self.pos + 1 : self.pos + self.seq_length + 1]\n",
    "        \n",
    "        loss, gradients, self.h = back_propagation(params, x, y, self.h)\n",
    "        Δs = self.optimizer.send(gradients)\n",
    "        for par, Δ in zip(params, Δs):\n",
    "            par += Δ\n",
    "        self.pos += self.seq_length\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "continuous-colorado",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(params, seed_idx, num_samples, h0=None):\n",
    "    x = np.zeros((num_samples + 1, vocab_size), dtype=float)\n",
    "    x[0, seed_idx] = 1\n",
    "    idx = np.empty(num_samples, dtype=int)\n",
    "    h_t = h0\n",
    "    for t in range(num_samples):\n",
    "        h_t, _, yhat_t = step(params, x[t, :], h_t)\n",
    "        # draw from output distribution\n",
    "        idx[t] = np.random.choice(range(vocab_size), p=yhat_t.ravel())        \n",
    "        x[t + 1, idx[t]] = 1\n",
    "    chars = (idx_to_char[i] for i in idx)\n",
    "    return str.join('', chars)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floral-scott",
   "metadata": {},
   "source": [
    "## Parameter initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "extensive-house",
   "metadata": {},
   "outputs": [],
   "source": [
    "h_size = 100 # number of units in hidden layer\n",
    "seq_length = 25 # number of steps to unroll the RNN for\n",
    "max_train_step = 500000\n",
    "\n",
    "# initialize model parameters\n",
    "Wxh = np.random.randn(h_size, vocab_size) * 0.01 \n",
    "Whh = np.random.randn(h_size, h_size) * 0.01\n",
    "Why = np.random.randn(vocab_size, h_size) * 0.01 \n",
    "bh = np.zeros((h_size, 1)) # hidden layer bias\n",
    "by = np.zeros((vocab_size, 1)) # readout layer bias\n",
    "params = Wxh, Whh, Why, bh, by\n",
    "\n",
    "trainer = Trainer(data, seq_length)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "scientific-birmingham",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en:\n",
      "Now the\n",
      "gins day my feirs\n",
      "Then his\n",
      "ane conspeck you are toy;\n",
      "I genal that is your panes sornact atill: and tethis your nase\n",
      "Jalichis.\n",
      "\n",
      "Sepoth I pear threeds you show, by thee, sir, as lo; the fart\n",
      "\n",
      "train step 50000, loss: 36\n",
      "--------------------------------------------------------------------------------\n",
      "ed furghter thou my sword?\n",
      "\n",
      "HASTINGS:\n",
      "Alasion,.\n",
      "\n",
      "BUCKINGHAM:\n",
      "Then, when, my ramms, by ent since to him how: my soife! Beery-not is that well the consen\n",
      "Thy will tween not thee this don''d now by pack \n",
      "\n",
      "train step 100000, loss: 19\n",
      "--------------------------------------------------------------------------------\n",
      "ence is thus broke me thou all neadsh a our my deave our peod I basping.\n",
      "Hare the head?\n",
      "\n",
      "Lake, Bolainster eign our toops made say and sence,\n",
      "And did the lightroy the radious his laintners of thy may g\n",
      "\n",
      "train step 150000, loss: 51\n",
      "--------------------------------------------------------------------------------\n",
      "ent\n",
      "Unill the too my leave, and tike on't ther, who none:\n",
      "Oument me?\n",
      "\n",
      "ROMEO:\n",
      "Make thou stalt my vonit.\n",
      "Some one with thee:\n",
      "That.\n",
      "\n",
      "LADY CAPULET:\n",
      "Gaussing day shall may I sgant with slail thou, then my \n",
      "\n",
      "train step 200000, loss: 30\n",
      "--------------------------------------------------------------------------------\n",
      "en:\n",
      "Why doward's cresiffe of math, and adverted of Ligle Clipid?\n",
      "As fearful one oresed\n",
      "With he sucess by all him was me faith, so Clarest live I am igns obe talh my rosed my rage.\n",
      "\n",
      "CLARENCE:\n",
      "I rualard\n",
      "\n",
      "train step 250000, loss: 50\n",
      "--------------------------------------------------------------------------------\n",
      "end underd and looks unforts:\n",
      "Let endorkn andle end friess of not thou confure it.\n",
      "\n",
      "I'TS OF\n",
      "OARY:\n",
      "Not his kings his haste\n",
      "That hast loons feach\n",
      "To rongy of Wiving it.\n",
      "\n",
      "FLORIZEL:\n",
      "As yet time threa; you\n",
      "\n",
      "train step 300000, loss: 47\n",
      "--------------------------------------------------------------------------------\n",
      "en.\n",
      "\n",
      "TALIDYO:\n",
      "Valicy be us vise you to couttry this carriceniest, so colter\n",
      "Us too, fool me?\n",
      "\n",
      "ESCALUS:\n",
      "The so-trumise, for he\n",
      "shall do mostice concadgees it is his goblithes;\n",
      "For shall we to be asteri\n",
      "\n",
      "train step 350000, loss: 38\n",
      "--------------------------------------------------------------------------------\n",
      "ar, 'tish and show you doely:\n",
      "So, the neielte. Signes then.\n",
      "\n",
      "HORTENSIO: Bodners best to at to be so counter wine,\n",
      "God my mistres and brayoales, her excersuase.\n",
      "There im his princes ane though him towa\n",
      "\n",
      "train step 400000, loss: 25\n",
      "--------------------------------------------------------------------------------\n",
      "els; commingic holder of own as for what mores saik, you still it she latting.\n",
      "\n",
      "CERTIUS:\n",
      "This of youth,\n",
      "Your seve you wounds you here? deeds of it speaking your other\n",
      "Om ruther-hand;\n",
      "Man you\n",
      " me spein\n",
      "\n",
      "train step 450000, loss: 47\n",
      "--------------------------------------------------------------------------------\n",
      "er world; that no douburingssian:\n",
      "No bey,\n",
      "He sent he requile of the keept,\n",
      "We Call.\n",
      "\n",
      "PRINCE:\n",
      "And that, not litters loss Ancerst.\n",
      "\n",
      "ISCUSTIS OF YORK:\n",
      "The cturst like uncle,\n",
      "To word to thou fill:\n",
      "Than yo\n",
      "\n",
      "train step 500000, loss: 40\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "while trainer.step < max_train_step:\n",
    "    loss = trainer.train(params)\n",
    "    if trainer.step % (max_train_step//10) == 0:\n",
    "        sample_text = sample(params, 0, 200)\n",
    "        print(sample_text)\n",
    "        print()\n",
    "        print('train step {:d}, loss: {:.2g}'.format(trainer.step, loss))\n",
    "        print('-'*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-hello",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
