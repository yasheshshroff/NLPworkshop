{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## PyTorch Multilayer Perceptron\n",
    "\n",
    "Create a subclass of the `torch.nn.Module` class and override the `forward()` method. This method defines how the network processes input and produces output.\n",
    "\n",
    "The `MLP` class takes `input size`, `hidden_size`, and `output_size` as arguments. \n",
    "The `__init__()` method creates two fully-connected layers (`nn.Linear`) and a ReLU activation function (`nn.ReLU`). \n",
    "The `forward()` method applies these layers to the input data `x` and returns the output.\n",
    "\n",
    "To train this network, we define a loss function, choose an optimizer, and iterate over the input data to update the model's weights. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# create the network\n",
    "mlp = MLP(input_size=784, hidden_size=256, output_size=10)\n",
    "\n",
    "# generate some random input data\n",
    "x = torch.randn(64, 784)\n",
    "\n",
    "# feed the input data through the network\n",
    "output = mlp(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding code for optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "# Any copyright is dedicated to the Public Domain.\n",
    "# https://creativecommons.org/publicdomain/zero/1.0/\n",
    "\n",
    "# Written by Francois Fleuret <francois@fleuret.org>\n",
    "\n",
    "from torch import Tensor\n",
    "\n",
    "import sys\n",
    "\n",
    "def exception_hook(exc_type, exc_value, tb):\n",
    "    r'''Hacks the call stack message to show all the local variables in\n",
    "    case of RuntimeError or ValueError, and prints tensors as shape,\n",
    "    dtype and device.\n",
    "\n",
    "    '''\n",
    "\n",
    "    repr_orig=Tensor.__repr__\n",
    "    Tensor.__repr__=lambda x: f'{x.size()}:{x.dtype}:{x.device}'\n",
    "\n",
    "    while tb:\n",
    "        print('--------------------------------------------------\\n')\n",
    "        filename = tb.tb_frame.f_code.co_filename\n",
    "        name = tb.tb_frame.f_code.co_name\n",
    "        line_no = tb.tb_lineno\n",
    "        print(f'  File \"{filename}\", line {line_no}, in {name}')\n",
    "        print(open(filename, 'r').readlines()[line_no-1])\n",
    "\n",
    "        if exc_type in { RuntimeError, ValueError }:\n",
    "            for n,v in tb.tb_frame.f_locals.items():\n",
    "                print(f'  {n} -> {v}')\n",
    "\n",
    "        print()\n",
    "        tb = tb.tb_next\n",
    "\n",
    "    Tensor.__repr__=repr_orig\n",
    "\n",
    "    print(f'{exc_type.__name__}: {exc_value}')\n",
    "\n",
    "sys.excepthook = exception_hook\n",
    "\n",
    "######################################################################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import random\n",
    "\n",
    "# define the function to generate the data from\n",
    "def f(x):\n",
    "    return 3.2 * x**3 + 2 * x**2 + 3 * x\n",
    "\n",
    "# the amount of noise to add\n",
    "noise_scale = 0.1\n",
    "\n",
    "# the number of samples to generate\n",
    "num_samples = 1000\n",
    "\n",
    "\n",
    "# generate the input values\n",
    "x_values = [random.uniform(-1, 1) for _ in range(num_samples)]\n",
    "\n",
    "# evaluate the function at each input value\n",
    "y_values = [f(x) + random.gauss(0, noise_scale) for x in x_values]\n",
    "\n",
    "# split the data into train and test sets\n",
    "split_index = int(0.8 * num_samples)\n",
    "x_train, y_train = x_values[:split_index], y_values[:split_index]\n",
    "x_test, y_test = x_values[split_index:], y_values[split_index:]\n",
    "\n",
    "# define the dataset\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, x_values, y_values):\n",
    "        self.data = x_values\n",
    "        self.labels = y_values\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # this returns a tuple of the input and label data for a sample, rather than a single tensor containing the data\n",
    "        return self.data[index], self.labels[index] \n",
    "        # To return a single element\n",
    "        # return torch.tensor([self.data[index], self.labels[index]])\n",
    "\n",
    "# define the neural network\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        print(f\"Shape of x: {x.shape}\")\n",
    "        x = self.fc1(x) #torch.reshape(x,(64,1))\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# create the network, dataset, and data loader\n",
    "mlp = MLP(input_size=1, hidden_size=64, output_size=1)\n",
    "# mlp = MLP(input_size=784, hidden_size=256, output_size=10)\n",
    "train_dataset = MyDataset(x_train, y_train)\n",
    "train_data_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataset = MyDataset(x_test, y_test)\n",
    "test_data_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(mlp.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# train the network\n",
    "for epoch in range(10):\n",
    "    for data, labels in train_data_loader:\n",
    "        # feed the data through the network\n",
    "        output = mlp(data)\n",
    "        \n",
    "        # calculate the loss\n",
    "        loss = criterion(output, labels)\n",
    "        \n",
    "        # backpropagate the gradients\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MyDataset(x_test, y_test)\n",
    "\n",
    "# print the length and shape of the dataset\n",
    "print(f\"The dataset has length {len(dataset)}\")\n",
    "print(f\"The value of the first sample in the dataset is {dataset[0]}\")\n",
    "print(f\"The dataset length is {len(dataset)}\")\n",
    "print(f\"The dataset width is {len(dataset[0])}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.arange(4.)\n",
    "torch.reshape(a, (2, 2))\n",
    "b = torch.tensor([[0, 1], [2, 3]])\n",
    "torch.reshape(b, (-1,))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.reshape(a, (2,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the quality of the trained MLP model, you can use the test dataset to evaluate the model's performance on unseen data. One way to do this is to iterate over the test data, making predictions with the model and comparing the predictions to the true labels. You can then calculate various performance metrics, such as the mean squared error (MSE) or the mean absolute error (MAE), to assess the quality of the model's predictions.\n",
    "\n",
    "Here is an example of how you might evaluate the trained MLP model using the test dataset:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the list of predicted values and the list of true values\n",
    "predicted_values = []\n",
    "true_values = []\n",
    "\n",
    "# iterate over the test data\n",
    "for inputs, labels in test_data_loader:\n",
    "    # make predictions using the model\n",
    "    outputs = mlp(inputs)\n",
    "    \n",
    "    # add the predicted values and true values to the list\n",
    "    predicted_values.extend(outputs)\n",
    "    true_values.extend(labels)\n",
    "\n",
    "# calculate the mean squared error\n",
    "mse = torch.mean((torch.tensor(predicted_values) - torch.tensor(true_values))**2)\n",
    "print(f\"Mean squared error: {mse}\")\n",
    "\n",
    "# calculate the mean absolute error\n",
    "mae = torch.mean(torch.abs(torch.tensor(predicted_values) - torch.tensor(true_values)))\n",
    "print(f\"Mean absolute error: {mae}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PyTorch - MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn # All the NN modules (ex. nn.Linear & loss functions)\n",
    "import torch.optim as optim # SGD, Adam\n",
    "import torch.nn.functional as F # Functions without parameters (activations)\n",
    "from torch.utils.data import DataLoader # dataset management w minibatch mgmt\n",
    "import torchvision.datasets as datasets # built-in datasets\n",
    "import torchvision.transforms as transforms # transformations on our dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating our fully connected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NN(nn.Module):\n",
    "    def __init__(self, input_size, num_classes):\n",
    "        super(NN, self).__init__() # call initialization of nn.Module\n",
    "        self.fc1 = nn.Linear(input_size, 25) # hidden layers are 25\n",
    "        self.fc2 = nn.Linear(25, num_classes)\n",
    "\n",
    "    def forward(self, x): # x: input to run forward method upon\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model to check transformation on random data\n",
    "x = torch.randn(32, 784) # batch size is 32, each image 784 pixels (28x28)\n",
    "model = NN(784, 10) # 10 classes\n",
    "print(model(x).shape) # should return 32x10 (prob across all numbers for each image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_classes = 10\n",
    "num_epochs = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "train_dataset = datasets.MNIST(root='dataset/', train=True, transform=transforms.ToTensor(), download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_dataset = datasets.MNIST(root='dataset/', train=False, transform=transforms.ToTensor(), download=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network initialization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NN(input_size=input_size, num_classes=num_classes).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loss function & optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "        data = data.to(device=device) # transfer tensor \n",
    "        target = targets.to(device=device)\n",
    "\n",
    "        print(data.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN from Scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.random import randn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "  # A many-to-one Vanilla Recurrent Neural Network.\n",
    "  # \n",
    "  def __init__(self, input_size, output_size, hidden_size=64):\n",
    "    # Weights\n",
    "    self.Whh = randn(hidden_size, hidden_size) / 1000\n",
    "    self.Wxh = randn(hidden_size, input_size) / 1000\n",
    "    self.Why = randn(output_size, hidden_size) / 1000\n",
    "\n",
    "    # Biases\n",
    "    self.bh = np.zeros((hidden_size, 1))\n",
    "    self.by = np.zeros((output_size, 1))\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    '''\n",
    "    Perform a forward pass of the RNN using the given inputs.\n",
    "    Returns the final output and hidden state.\n",
    "    - inputs is an array of one hot vectors with shape (input_size, 1).\n",
    "    '''\n",
    "    h = np.zeros((self.Whh.shape[0], 1))\n",
    "\n",
    "    self.last_inputs = inputs\n",
    "    self.last_hs = { 0: h }\n",
    "\n",
    "    # Perform each step of the RNN\n",
    "    for i, x in enumerate(inputs):\n",
    "      h = np.tanh(self.Wxh @ x + self.Whh @ h + self.bh)\n",
    "      self.last_hs[i + 1] = h\n",
    "\n",
    "    # Compute the output\n",
    "    y = self.Why @ h + self.by\n",
    "\n",
    "    return y, h\n",
    "    \n",
    "  def backprop(self, d_y, learn_rate=2e-2):\n",
    "    '''\n",
    "    Perform a backward pass of the RNN.\n",
    "    - d_y (dL/dy) has shape (output_size, 1).\n",
    "    - learn_rate is a float.\n",
    "    '''\n",
    "    n = len(self.last_inputs)\n",
    "\n",
    "    # Calculate dL/dWhy and dL/dby.\n",
    "    d_Why = d_y @ self.last_hs[n].T\n",
    "    d_by = d_y\n",
    "\n",
    "    # Initialize dL/dWhh, dL/dWxh, and dL/dbh to zero.\n",
    "    d_Whh = np.zeros(self.Whh.shape)\n",
    "    d_Wxh = np.zeros(self.Wxh.shape)\n",
    "    d_bh = np.zeros(self.bh.shape)\n",
    "\n",
    "    # Calculate dL/dh for the last h.\n",
    "    # dL/dh = dL/dy * dy/dh\n",
    "    d_h = self.Why.T @ d_y\n",
    "\n",
    "    # Backpropagate through time.\n",
    "    for t in reversed(range(n)):\n",
    "      # An intermediate value: dL/dh * (1 - h^2)\n",
    "      temp = ((1 - self.last_hs[t + 1] ** 2) * d_h)\n",
    "\n",
    "      # dL/db = dL/dh * (1 - h^2)\n",
    "      d_bh += temp\n",
    "\n",
    "      # dL/dWhh = dL/dh * (1 - h^2) * h_{t-1}\n",
    "      d_Whh += temp @ self.last_hs[t].T\n",
    "\n",
    "      # dL/dWxh = dL/dh * (1 - h^2) * x\n",
    "      d_Wxh += temp @ self.last_inputs[t].T\n",
    "\n",
    "      # Next dL/dh = dL/dh * (1 - h^2) * Whh\n",
    "      d_h = self.Whh @ temp\n",
    "\n",
    "    # Clip to prevent exploding gradients.\n",
    "    for d in [d_Wxh, d_Whh, d_Why, d_bh, d_by]:\n",
    "      np.clip(d, -1, 1, out=d)\n",
    "\n",
    "    # Update weights and biases using gradient descent.\n",
    "    self.Whh -= learn_rate * d_Whh\n",
    "    self.Wxh -= learn_rate * d_Wxh\n",
    "    self.Why -= learn_rate * d_Why\n",
    "    self.bh -= learn_rate * d_bh\n",
    "    self.by -= learn_rate * d_by\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = {\n",
    "  'good': True,\n",
    "  'bad': False,\n",
    "  'happy': True,\n",
    "  'sad': False,\n",
    "  'not good': False,\n",
    "  'not bad': True,\n",
    "  'not happy': False,\n",
    "  'not sad': True,\n",
    "  'very good': True,\n",
    "  'very bad': False,\n",
    "  'very happy': True,\n",
    "  'very sad': False,\n",
    "  'i am happy': True,\n",
    "  'this is good': True,\n",
    "  'i am bad': False,\n",
    "  'this is bad': False,\n",
    "  'i am sad': False,\n",
    "  'this is sad': False,\n",
    "  'i am not happy': False,\n",
    "  'this is not good': False,\n",
    "  'i am not bad': True,\n",
    "  'this is not sad': True,\n",
    "  'i am very happy': True,\n",
    "  'this is very good': True,\n",
    "  'i am very bad': False,\n",
    "  'this is very sad': False,\n",
    "  'this is very happy': True,\n",
    "  'i am good not bad': True,\n",
    "  'this is good not bad': True,\n",
    "  'i am bad not good': False,\n",
    "  'i am good and happy': True,\n",
    "  'this is not good and not happy': False,\n",
    "  'i am not at all good': False,\n",
    "  'i am not at all bad': True,\n",
    "  'i am not at all happy': False,\n",
    "  'this is not at all sad': True,\n",
    "  'this is not at all happy': False,\n",
    "  'i am good right now': True,\n",
    "  'i am bad right now': False,\n",
    "  'this is bad right now': False,\n",
    "  'i am sad right now': False,\n",
    "  'i was good earlier': True,\n",
    "  'i was happy earlier': True,\n",
    "  'i was bad earlier': False,\n",
    "  'i was sad earlier': False,\n",
    "  'i am very bad right now': False,\n",
    "  'this is very good right now': True,\n",
    "  'this is very sad right now': False,\n",
    "  'this was bad earlier': False,\n",
    "  'this was very good earlier': True,\n",
    "  'this was very bad earlier': False,\n",
    "  'this was very happy earlier': True,\n",
    "  'this was very sad earlier': False,\n",
    "  'i was good and not bad earlier': True,\n",
    "  'i was not good and not happy earlier': False,\n",
    "  'i am not at all bad or sad right now': True,\n",
    "  'i am not at all good or happy right now': False,\n",
    "  'this was not happy and not good earlier': False,\n",
    "}\n",
    "\n",
    "test_data = {\n",
    "  'this is happy': True,\n",
    "  'i am good': True,\n",
    "  'this is not happy': False,\n",
    "  'i am not good': False,\n",
    "  'this is not bad': True,\n",
    "  'i am not sad': True,\n",
    "  'i am very good': True,\n",
    "  'this is very bad': False,\n",
    "  'i am very sad': False,\n",
    "  'this is bad not good': False,\n",
    "  'this is good and happy': True,\n",
    "  'i am not good and not happy': False,\n",
    "  'i am not at all sad': True,\n",
    "  'this is not at all good': False,\n",
    "  'this is not at all bad': True,\n",
    "  'this is good right now': True,\n",
    "  'this is sad right now': False,\n",
    "  'this is very bad right now': False,\n",
    "  'this was good earlier': True,\n",
    "  'i was not happy and not good earlier': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# from rnn import RNN\n",
    "# from data import train_data, test_data\n",
    "\n",
    "# Create the vocabulary.\n",
    "vocab = list(set([w for text in train_data.keys() for w in text.split(' ')]))\n",
    "vocab_size = len(vocab)\n",
    "print('%d unique words found' % vocab_size)\n",
    "\n",
    "# Assign indices to each word.\n",
    "word_to_idx = { w: i for i, w in enumerate(vocab) }\n",
    "idx_to_word = { i: w for i, w in enumerate(vocab) }\n",
    "# print(word_to_idx['good'])\n",
    "# print(idx_to_word[0])\n",
    "\n",
    "def createInputs(text):\n",
    "  '''\n",
    "  Returns an array of one-hot vectors representing the words in the input text string.\n",
    "  - text is a string\n",
    "  - Each one-hot vector has shape (vocab_size, 1)\n",
    "  '''\n",
    "  inputs = []\n",
    "  for w in text.split(' '):\n",
    "    v = np.zeros((vocab_size, 1))\n",
    "    v[word_to_idx[w]] = 1\n",
    "    inputs.append(v)\n",
    "  return inputs\n",
    "\n",
    "def softmax(xs):\n",
    "  # Applies the Softmax Function to the input array.\n",
    "  return np.exp(xs) / sum(np.exp(xs))\n",
    "\n",
    "# Initialize our RNN!\n",
    "rnn = RNN(vocab_size, 2)\n",
    "\n",
    "def processData(data, backprop=True):\n",
    "  '''\n",
    "  Returns the RNN's loss and accuracy for the given data.\n",
    "  - data is a dictionary mapping text to True or False.\n",
    "  - backprop determines if the backward phase should be run.\n",
    "  '''\n",
    "  items = list(data.items())\n",
    "  random.shuffle(items)\n",
    "\n",
    "  loss = 0\n",
    "  num_correct = 0\n",
    "\n",
    "  for x, y in items:\n",
    "    inputs = createInputs(x)\n",
    "    target = int(y)\n",
    "\n",
    "    # Forward\n",
    "    out, _ = rnn.forward(inputs)\n",
    "    probs = softmax(out)\n",
    "\n",
    "    # Calculate loss / accuracy\n",
    "    loss -= np.log(probs[target])\n",
    "    num_correct += int(np.argmax(probs) == target)\n",
    "\n",
    "    if backprop:\n",
    "      # Build dL/dy\n",
    "      d_L_d_y = probs\n",
    "      d_L_d_y[target] -= 1\n",
    "\n",
    "      # Backward\n",
    "      rnn.backprop(d_L_d_y)\n",
    "\n",
    "  return loss / len(data), num_correct / len(data)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(1000):\n",
    "  train_loss, train_acc = processData(train_data)\n",
    "\n",
    "  if epoch % 100 == 99:\n",
    "    print('--- Epoch %d' % (epoch + 1))\n",
    "    print('Train:\\tLoss %.3f | Accuracy: %.3f' % (train_loss, train_acc))\n",
    "\n",
    "    test_loss, test_acc = processData(test_data, backprop=False)\n",
    "    print('Test:\\tLoss %.3f | Accuracy: %.3f' % (test_loss, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "3ad933181bd8a04b432d3370b9dc3b0662ad032c4dfaa4e4f1596c548f763858"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
